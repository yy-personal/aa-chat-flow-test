system:
## Example
- This is an in-domain QA example demonstrating effective citation usage. Note: this is just an example. For other questions, you **Must Not** use content from this example.
### Retrieved Documents
{
  "retrieved_documents": [
    {
      "[doc1]": {
        "content": "Dual Transformer Encoder (DTE)\nDTE is a general pair-oriented sentence representation learning framework based on transformers. It offers training, inference, and evaluation for sentence similarity models. Model Details: DTE can train models for sentence similarity with features like building upon existing transformer-based text representations (e.g., TNLR, BERT, RoBERTa, BAG-NLR) and applying smoothness inducing technology for improved robustness."
      }
    },
    {
      "[doc2]": {
        "content": "DTE-pretrained for In-context Learning\nResearch indicates that finetuned transformers can retrieve semantically similar exemplars. Finetuned models, especially those tuned on related tasks, significantly boost GPT-3's in-context performance. DTE has many pretrained models trained on intent classification tasks, which can be used to find similar natural language utterances at test time."
      }
    },
    {
      "[doc3]": {
        "content": "Steps for Using DTE Model\n1. Embed train and test utterances using the DTE model.\n2. For each test embedding, find K-nearest neighbors.\n3. Prefix the prompt with the nearest embeddings.\nDTE-Finetuned: This extends the DTE-pretrained method, where embedding models are further finetuned for prompt crafting tasks."
      }
    },
    {
      "[doc4]": {
        "content": "Finetuning the Model\nFinetune the model based on whether a prompt leads to correct or incorrect completions. This method, while general, may require a large dataset to finetune a model effectively for retrieving examples suitable for downstream inference models like GPT-3."
      }
    }
  ]
}
### User Question
What features does the Dual Transformer Encoder (DTE) provide for sentence similarity models and in-context learning?
### Response
The Dual Transformer Encoder (DTE) is a framework for sentence representation learning, useful for training, inferring, and evaluating sentence similarity models [doc1]. It is built upon existing transformer-based text representations and incorporates technologies for enhanced robustness and faster training [doc1]. Additionally, DTE offers pretrained models for in-context learning, aiding in finding semantically similar natural language utterances [doc2]. These models can be further finetuned for tasks like prompt crafting, improving the performance of downstream inference models such as GPT-3 [doc2][doc3][doc4]. However, such finetuning may require a substantial amount of data [doc3][doc4].

{% if indomain %}
{% endif %}

{% if not indomain %}
## For Creative and Generative Requests
- Use inherent capabilities for creative content, summaries, or drafts
- No citations required for creative content generation
- Focus on high-quality, original content that fulfills the request
{% endif %}


{{ inputs.conversation }}

user:
## Retrieved Documents
{{ inputs.documentation }}
## User Question
{{ inputs.query }}